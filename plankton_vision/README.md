# Phytoplankton classification with deep networks

### Using vector and image data generated by IFCB (Imaging FlowCytobot) to train a taxonomic classifier of phytoplankton in the North Atlantic

###### Examples from dominant high level plankton categories 
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/class-examples-title-asset.png)

### I: Desription of raw data & distribution of class examples 

#### ia: Loading and resolving datasets
The inputs for the classification networks described in this repo consist of image files and accompanying metadata. The image files are of a variable dimension with RGB channels and the metadata is a vector that has 16 elements. The data were provided accross three separate files: 

    * a zipfile of all images (NAAMES.zip)
    * a taxonomic reference file (taxonomic_grouping_v3.csv)
    * a metadata file (IFCB_CSV/master.csv)
    
The image files in the NAAMES directory are named according to the convention FILE_ID + _ + CATEGORY_PRETTIFIED. These strings can then be parsed out and joined to the other datasets (metadata joins on FILE_ID and taxonomic reference joins on CATEGORY_PRETTIFIED). Extracts from the raw taxonomy reference and metadata files are shown below respectively: 
###### Taxonomy reference
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/assets_1_taxonomy.png)
###### Metadata sample
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/metadata-sample-asset.png)
These reference data are then joined to the list of image files in the NAAMES directory, which then gives the full dataset used in the analysis (shown below after join): 
###### Joined dataset for analysis
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/joined-data-assets.png)

#### ib: Adding flags for missing metadata & coercing examples with no associated CATEGORY_GROUPED
When the datasets have been linked to the file directory, there are sets of examples for which there is no corresponding metadata in the master file, and for which there is no associated CATEGORY_GROUPED in the taxonomy reference. Pictured below (left) is a frequency table of the CATEGORY_PRITIFIED values for which there is no CATEGORY_GROUPED in the taxonomy reference. Pictured below on the right is a refernce dictionary I made to manually coerce the CATEGORY_GROUPED assignment of the examples. The Bacillariophyta category has been changed to Diatom in the current version of the analysis. In total there are 220,896 records of 1,998,900 which needed to be manually assigned to CATEGORY_GROUPED. 
###### Coerce CATEGORY_GROUPED reference dictionary and frequency table
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/assets-no-group-coerce.png)

The files missing metadata have been excuded from the analysis. In total there are 132,781 records of 1,990,900 with no corresponding entry in the metadata file. Files missing metadata are shown below in a historgram over CATEGORY_GROUPED, compared with a historgram of the raw data over the same domain. The plots illustrate that the class distribution of files missing metadata is approximatey the same as the class distribution in the raw data, presumably meaning that it is a random subset. 
###### Distribution of files without metadata (bottom) compared with class distribution in the raw data (top) 
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/histogram-all-data-and-missingmd-asset.png)

### II: Building datasets for classification networks

#### iia: Exclusions to raw data
Before doing any upsampling or preprocessing on the data, I make two exclusions: 1) remove all examples missing metadata, 2) remove all examples for which the equivalent spherical diameter by area (ESDA) is less than or equal to 7.1 um. All images in this dataset with a value below this threshold were intended to be labeled as Unicellular, though when this exclusion is applied there is some very minor loss of data from the valid classes.
###### Plot of examples excluded by ESDA threshold (99.5% exclusion of invalid data, 0.54% exclusion of valid data)
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/esda-exclude-asset.png)

#### iib: Basic note on train / validation division of data and samping methodology 
Since the ESDA exclusion does eliminate some valid examples, the impact will be slightly different on training and testing subsets of the data. I use 85% of the data for training, and 15% for testing. The distribution of class examples in the testing data is aproximately the same as the distribution in the raw data. Having imbalanced classes in the training data can cause undesireable and trivial learning that takes advantage of the probability that any example (xn : set(x0..xn)) is non uniform across the classes. At this point in development, I have just upsampled the under-represented classes (both at the CATEGORY_GROUPED level and CATEGORY_PRITTIFIED level). This has helped a ton in model training, and I think its definitely worthwhile to explore more sophisticated approaches to sampling (like SMOTE or using Autoencoders) to see if it will improve performance on real data.

### III: Network designs & performance summary 

#### iiia: Model 1A
##### Model 1A overview
The first algorithm I used for this analysis splits the classification task into a sequential two step process: 

   * PART I: Binary classication network to exclude invalid or non-planktonic data from the raw datastream
   * PART II: Multi-class classification network over the CATEGORY_GROUPED taxonomy 

Breifly, the reasoning behind using two networks as opposed to a single network is based on a (potentially wrong) hueristic that: 

   * the task of excluding invalid images might rely more on the numeric vector data than the image pixel matricies, which is based on the observation that ESDA thresholding removed nearly 400k images with a single parameter in the vector data.
   * the objective function for either task might be disigned to optimise different sorts of error (ex. precision and recall) based on how we want to use the model (ie we really dont want to throw out any valid plankton images in the binary model, so change the loss function to penalize that sort of misclasification more). 

I am going to try both this sequential approach and single-network approach to see which performs better. In terms of the network structure, since we have both numeric data and image data, it makes sense to employ the use of a multi-input model which accepts an image and a metadata vector for each example in the data. In both the binary and multi-class networks, the vector data is input to a multi layer perceptron network (MLP) and the image data is input to a convolutional nueral network (CNN). After the output activations from the last layer of the CNN and MLP nets are then concatenated and fed through several more dense layers and then output as a probability distribution over the classes. The basic components of the two networks are pictured below: 

##### Model 1A overview: Binary network structure

###### Input layer 128 * 128 * 3 (RGB) and example convolution (ReLU, batch normalization, max pooling & dropout)
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/cnn-binary-asset.png)
 
###### MLP network branch and CNN branch concatenation
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/mlp-cnn-concat-binary-asset.png)

###### Combined layers and output activation 
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/combined-stream-binary-asset.png)


# *** RESULTS BEGIN (will add text to accompany figures & samples of commonly misclassified images tomorrow) ***

##### Model 1A overview: Binary network performance
Both of the networks for this analysis were trained using the same (85%) subset of the raw data. The trained models and esda thresholding are run sequentially on the testing sample (15%) so it is exactly like running the code on a random sample of the unlabeled data. The raw sample has 299,835 examples, and after making the esda and missing-metadata exclusions, we are left with 215,357 examples. The binary classification model is run on this subset of the data, and excludes every example the model predicts as belonging to the 'not plankton' (which is really non-living) group. This model has overall accuracy of 96%, with 97% or plankton correctly labeled, and 92% of not plankton labeled correctly. The distribution of classes in the data before and after the binary network is used to exclude examples is pictured below:

###### Distribution of class examples in the testing data 
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/testing-data-distribution-asset.png)

###### Distribution of class examples in the testing data after examples classifed as not plankton are removed 
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/testing-data-after-binary-classification-asset.png)

As depicted in the histograms above, this network does very well at excluding the majority of the invalid examples in the data. The accuracy of the binary classifier for the individual higher level groups is shown below for all images in the 1) plankton and 2) not plankton groups: 

###### Relative/normalized percent error (left) and raw percent error (right) for plankton classes 
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/binary-class-by-high-group-matrix.png)

###### Relative/normalized percent error (left) and raw percent error (right) for plankton classes 
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/not-plankton-binary-confusion-matrices-asset.png)

###### Mean output probabilities (binary 0 = not plankton, 1 = plankton) for all higher level classes in the data
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/binary-probability-high-group-asset.png)

##### Model 1A overview: Multi-class network 

###### Confusion matrix for all classes in the multi-class network  
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/confusion-mat-multi-all-asset.png)

###### Mean output probability vectors for each of the classes in the multi-class network  
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/mean-probability-dist-slc-asset.png)


##### Model 1A overview: Multi-class network 






Might want to see what the performance of the MLP network is on the data, and then see what the performance of the cnn is on the data, and optimize those separately and then see if the combined model outperforms, or is you could be ensembling them differently in some way. 


### Introduction
There are a number of papers that use convolutional networks to classify plankton imagery (1)(2)(3). Several of these papers came out of submissions to the National Data Science Bowl 2015 Kaggle competition, which asked participants to classify over 100+ imbalanced classes of plankton images taken by a submersible camera. There is some diversity in the range of approaches taken by participants, though all ranking submissions used convolutional neural networks. The papers I based this analysis on built models with VGG-like architectures:

   * 6+ convolutional layers
   * ReLU activations (or variant)
   * Pooling layers (Max and variants)
   * Fully connected dense layers
   * Regularization (dropout most common)
   * Fixed stride of 1
   * Layer by layer increasing number of 3*3 filters








