# Phytoplankton classification with deep neural networks
## Using vector and image data generated by IFCB (Imaging FlowCytobot) to train a taxonomic classifier of phytoplankton in the North Atlantic

   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/class-examples-title-asset.png)

### ** Purpose of repository / disclaimer (for my fans)
This READMe document is intended to reflect the current state of the work stream and provide updates to stakeholders in a centralized location. As such it does not represent a completed project, and correspondingly the code is unstable and cannot be pulled as ready-to-use software. 

### I: Desription of raw data & distribution of class examples 

#### ia: Loading and resolving datasets
The inputs for the classification networks described in this repo consist of image files and accompanying metadata vectors. The image files are of a variable dimension with RGB channels and the metadata vectors have 16 continuous numeric variables. The data were provided accross three separate files: 
    * a zipfile of all images (NAAMES.zip)
    * a taxonomic reference file (taxonomic_grouping_v3.csv)
    * a metadata file (IFCB_CSV/master.csv)
The image files in the NAAMES directory are named according to the convention FILE_ID + _ + CATEGORY_PRETTIFIED. These strings can then be parsed out and joined to the other datasets (metadata joins on FILE_ID and taxonomic reference joins on CATEGORY_PRETTIFIED). Extracts from the raw taxonomy reference and metadata files are shown below respectively: 
###### Taxonomy reference
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/assets_1_taxonomy.png)
###### Metadata sample
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/metadata-sample-asset.png)
These reference data are then joined to the list of image files in the NAAMES directory, which then gives the full dataset used in the analysis (shown below after join): 
###### Joined dataset for analysis
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/joined-data-assets.png)

#### ib: Adding flags for missing metadata & coercing examples with no associated CATEGORY_GROUPED
When the datasets have been linked to the file directory, there are sets of examples for which there is no corresponding metadata in the master file, and for which there is no associated CATEGORY_GROUPED in the taxonomy reference. Pictured below (left) is a frequency table of the CATEGORY_PRITIFIED values for which there is no CATEGORY_GROUPED in the taxonomy reference. Pictured below on the right is a refernce dictionary I made to manually coerce the CATEGORY_GROUPED assignment of the examples. The Bacillariophyta category has been changed to Diatom in the current version of the analysis. In total there are 220,896 records of 1,998,900 which needed to be manually assigned to CATEGORY_GROUPED. 
###### Coerce CATEGORY_GROUPED reference dictionary and frequency table
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/assets-no-group-coerce.png)
   
For the files missing metadata, there is no manual way to resolve the missing numeric data, and so they have just been excuded from the analysis. In total there are 132,781 records of 1,990,900 with no corresponding entry in the metadata file. Files missing metadata are shown below in a historgram over CATEGORY_GROUPED, compared with a historgram of the raw data over the same domain. The plots illustrate that the class distribution of files missing metadata is approximatey the same as the class distribution in the raw data, suggesting that it is a random subset. 
###### Distribution of files without metadata (bottom) compared with class distribution in the raw data (top) 
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/histogram-all-data-and-missingmd-asset.png)

### II: Building datasets for classification networks

#### iia: Exclusions to raw data
Before doing any upsampling or preprocessing on the data, we make two exclusions: 1) remove all examples missing metadata, 2) remove all examples for which the equivalent spherical diameter by area (ESDA) is less than or equal to 7.1 um, which corresponds to a 24 pixel cutoff at 3.4 pixels per micrometre. The threshold of 24 pixels / 7.1um is an approximate estimate of the threshold of minimum size below which the instrument data is not reliably quantifiable. The scientists who curated the dataset intended to label all images below this threshold as Unicellular, though when this exclusion is applied there is some very minor loss of data from the valid classes.
###### Plot of examples excluded by ESDA threshold (99.5% exclusion of invalid data, 0.54% exclusion of valid data)
   ![alt text](https://github.com/emmettFC/selected-projects/blob/master/plankton_vision/assets/esda-exclude-asset.png)

#### iib: Basic note on train / validation division of data and samping methodology 
Since the above exclusions do eliminate some valid examples, the impact of the exclusion will be slightly different on training and testing subsets of the data. Because of this, I split the data into training and testing subsets before I apply the ESDA & metadata exclusions, so that the results of the model reflect the expected performance on a totally random set of raw data from the IFCB. I use 85% of the data for training, and 15% for testing. The distribution of class examples in the testing data is aproximately the same as the distribution in the raw data. Having imbalanced classes in the training data can cause undesireable and trivial learning that takes advantage of the probability that any example (x : set(x)) is non uniform across the classes. At this point in development, I have just upsampled the underrepresented classes (both at the CATEGORY_GROUPED level and CATEGORY_PRITTIFIED level). This has helped a ton in model training, and so a more sophisticated approach to sampling (like SMOTE or using Autoencoders) will be implemented in further iterations of the analysis.  

### III: Network design & performance summary 

#### iiia: Model 1A



### Introduction
There are a number of papers that use convolutional networks to classify plankton imagery (1)(2)(3). Several of these papers came out of submissions to the National Data Science Bowl 2015 Kaggle competition, which asked participants to classify over 100+ imbalanced classes of plankton images taken by a submersible camera. There is some diversity in the range of approaches taken by participants, though all ranking submissions used convolutional neural networks. The papers I based this analysis on built models with VGG-like architectures:

   * 6+ convolutional layers
   * ReLU activations (or variant)
   * Pooling layers (Max and variants)
   * Fully connected dense layers
   * Regularization (dropout most common)
   * Fixed stride of 1
   * Layer by layer increasing number of 3*3 filters








