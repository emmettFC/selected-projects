{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Data Driven Yale: Analytical regionalization & Social media data\n",
    "        I: SOM clustering to determine initilizing grid params with DBI index\n",
    "'''\n",
    "\n",
    "# -- \n",
    "# dependancies \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json \n",
    "import geojson\n",
    "import matplotlib.pyplot as plt \n",
    "from descartes import PolygonPatch\n",
    "from pprint import pprint \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from neupy import algorithms, environment\n",
    "\n",
    "import pandas as pd \n",
    "from sklearn.cluster import k_means\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from utils import plot_2d_grid\n",
    "plt.style.use('ggplot')\n",
    "environment.reproducible()\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# io \n",
    "\n",
    "with open(\"./data/beijing.geojson\") as json_file:\n",
    "    json_data = geojson.load(json_file)\n",
    "\n",
    "df_weibo = pd.read_csv('./data/inLabelRegionPoints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# user defined functions\n",
    "\n",
    "def factors(x):\n",
    "    result = []\n",
    "    i = 1\n",
    "    while i*i <= x:\n",
    "        if x % i == 0:\n",
    "            result.append(i)\n",
    "            if x/i != i:\n",
    "                result.append(x/i)\n",
    "        i += 1\n",
    "    return result\n",
    "\n",
    "def makePQs(fs): \n",
    "    pqs = []\n",
    "    # f = facts[0]\n",
    "    ind = len(fs)\n",
    "    if ind % 2 != 0:\n",
    "        sq = fs[ind - 1]\n",
    "        pqs.append((sq, sq))\n",
    "        fs = fs[:-1]\n",
    "        ind = ind -1\n",
    "    i = 0\n",
    "    while i < ind: \n",
    "        pair = (fs[i], fs[i+1])\n",
    "        i += 2\n",
    "        # print(pair)\n",
    "        pqs.append(pair)\n",
    "    return pqs\n",
    "\n",
    "def validFacts(row): \n",
    "    dif = abs(row['f1'] - row['f2'])\n",
    "    rat = np.max((row['f1'], row['f2'])) / float(dif)\n",
    "    if rat < 2: \n",
    "        out = False \n",
    "    else: \n",
    "        out = True\n",
    "    return out\n",
    "\n",
    "def makePq(row, ind):\n",
    "    if ind == 'p':  \n",
    "        out = np.min((row['f1'], row['f2']))\n",
    "    elif ind == 'q':\n",
    "        out = np.max((row['f1'], row['f2']))\n",
    "    return out\n",
    "\n",
    "def makePq(row, ind):\n",
    "    if ind == 'p':  \n",
    "        out = np.min((row['f1'], row['f2']))\n",
    "    elif ind == 'q':\n",
    "        out = np.max((row['f1'], row['f2']))\n",
    "    return out\n",
    "\n",
    "def getDist(row): \n",
    "    p1 = (row['latp'], row['lonp']) \n",
    "    p2 = (row['lat'], row['lon']) \n",
    "    dst = distance.euclidean(p1, p2)\n",
    "    return dst\n",
    "\n",
    "def getDBIstd(grid_size, dfWB):\n",
    "    GRID_HEIGHT = grid_size['p']\n",
    "    GRID_WIDTH = grid_size['q']\n",
    "    wbDat = dfWB.sample(frac=0.10, replace=False)\n",
    "    wbArray = np.array(wbDat[['lon', 'lat']])\n",
    "    sofm = algorithms.SOFM(\n",
    "        n_inputs=2,\n",
    "        features_grid=(GRID_HEIGHT, GRID_WIDTH),\n",
    "        verbose=True,\n",
    "        shuffle_data=True,\n",
    "        distance='euclid',\n",
    "        learning_radius=2,\n",
    "        reduce_radius_after=20,\n",
    "        std=2,\n",
    "        reduce_std_after=50,\n",
    "        step=0.3,\n",
    "        reduce_step_after=50,\n",
    "    )\n",
    "    '''this takes 5 minutes'''\n",
    "    sofm.train(wbArray, epochs=20)\n",
    "    preds = sofm.predict(wbArray)\n",
    "    dfp = pd.DataFrame(preds)\n",
    "    dfp['label'] = dfp.idxmax(axis=1)\n",
    "    dfp.reset_index(inplace=True)\n",
    "    npd34 = sofm.weight\n",
    "    npd34Lats, npd34Lons = npd34[1], npd34[0]\n",
    "    coords = [] \n",
    "    for i in range(len(npd34Lats)): \n",
    "        lat, lon = npd34Lats[i], npd34Lons[i]\n",
    "        out = {\n",
    "            'lat' : lat, \n",
    "            'lon' : lon\n",
    "        }\n",
    "        coords.append(out)\n",
    "        # - \n",
    "    dfcnt = pd.DataFrame(coords)\n",
    "    dfcnt.reset_index(inplace=True)\n",
    "    dfcnt.columns = ['label', 'lat', 'lon']\n",
    "    dflab = pd.merge(dfp, dfcnt, on='label', how='outer')\n",
    "    wbdf = pd.DataFrame(wbArray, columns=['lonp', 'latp'])\n",
    "    wbdf.reset_index(inplace=True)\n",
    "    dfDB = pd.merge(dflab, wbdf, on='index', how='outer')\n",
    "    dfDB['euclidean_dist'] = dfDB.apply(lambda row: getDist(row), axis=1)\n",
    "    average_distance = dfDB.groupby('label').agg({'euclidean_dist' : np.std})\n",
    "    average_distance.reset_index(inplace=True)\n",
    "    dbframe = pd.merge(average_distance, dfcnt, on='label', how='outer')\n",
    "    Ri = []\n",
    "    for i in range(len(dbframe)): \n",
    "        Rij = [] \n",
    "        cnt = dbframe.iloc[i]\n",
    "        p1 = (cnt['lat'], cnt['lon']) \n",
    "        mean_dist = cnt['euclidean_dist']\n",
    "        for j in range(len(dbframe)): \n",
    "            if j != i: \n",
    "                cnt2 = dbframe.iloc[j]\n",
    "                p2 = (cnt2['lat'], cnt2['lon'])\n",
    "                mean_dist2 = cnt2['euclidean_dist']\n",
    "                r = (mean_dist + mean_dist2) / distance.euclidean(p1, p2)\n",
    "                Rij.append(r)\n",
    "                # - \n",
    "        Ri.append(max(Rij))\n",
    "        # - \n",
    "    print('found dbi for grid_size')\n",
    "    dbi = np.mean(Ri)\n",
    "    return dbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# get hueristic for general shape of intitializing grid\n",
    "'''this hueristic is based on logic from frias martinez'''\n",
    "\n",
    "maxLon, minLon = np.max(df_weibo['lon']), np.min(df_weibo['lon'])\n",
    "maxLat, minLat = np.max(dfWB['lat']), np.min(dfWB['lat'])\n",
    "\n",
    "corners = [(minLon, maxLat), (maxLon, maxLat), (maxLon, minLat), (minLon, minLat)]\n",
    "dfCorners = pd.DataFrame(corners)\n",
    "dfCorners.columns = ['lon', 'lat']\n",
    "plt.scatter(dfCorners['lon'], dfCorners['lat'])\n",
    "plt.show()\n",
    "\n",
    "width = maxLon - minLon\n",
    "height = maxLat - minLat\n",
    "ratio = np.float(height) /np.float(width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# get factors of all desired grid sizes\n",
    "\n",
    "N = []\n",
    "for i in range(10, 101): \n",
    "    N.append(i)\n",
    "\n",
    "facts = [factors(i) for i in N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --\n",
    "# make pairs of valid factors based on N\n",
    "\n",
    "pQs = []\n",
    "for i in range(len(facts)): \n",
    "    print(i)\n",
    "    pQs.extend(makePQs(facts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# exlude factor pairs where one is less than half of the other\n",
    "\n",
    "df_factor = pd.DataFrame(pQs)\n",
    "df_factor.columns = ['f1', 'f2']\n",
    "df_factor = df_factor.loc[(df_factor['f1'] != 1) & (df_factor['f2'] != 1)]\n",
    "\n",
    "df_factor['valid'] = df_factor.apply(lambda row: validFacts(row), axis=1)\n",
    "valid = df_factor.loc[df_factor['valid'] == True]\n",
    "valid['checkSum'] = valid.apply(lambda row: row['f1'] * row['f2'], axis=1)\n",
    "\n",
    "valid['p'] = valid.apply(lambda row: makePq(row, 'p'), axis=1)\n",
    "valid['q'] = valid.apply(lambda row: makePq(row, 'q'), axis=1)\n",
    "valid['checkAlign'] = valid.apply(lambda row: row['p'] <= row['q'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# run SOM and DBI for different valid initializing grid dims\n",
    "\n",
    "valid.reset_index(inplace=True)\n",
    "\n",
    "outDicts = []\n",
    "for i in range(len(valid)): \n",
    "    print('evaluating ' + str(i) + ' of ' + str(len(valid)) + 'initializing grids')\n",
    "    instance = valid.iloc[i][['index','p', 'q']]\n",
    "    dbi = getDBIstd(instance, df_weibo)\n",
    "    outDicts.append({\n",
    "            'index' : instance['index'], \n",
    "            'p' : instance['p'], \n",
    "            'q' : instance['q'],\n",
    "            'dbi' : dbi\n",
    "        })\n",
    "\n",
    "dfDBI = pd.DataFrame(outDicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# save DBI scores to file for reference\n",
    "'''has already been run; can be tabbed back in to make local copy'''\n",
    "\n",
    "# dfDBI.to_csv('./dbi-scores-for-pq-inits-std.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# plot DB index over N and save image \n",
    "\n",
    "dfDBI.head() \n",
    "dfDBI['grid_size'] = dfDBI.apply(lambda row: row['p'] * row['q'], axis=1)\n",
    "\n",
    "min_score = dfDBI.dbi.min()\n",
    "min_init = dfDBI.loc[dfDBI['dbi'] == min_score]\n",
    "x = min_init['grid_size']\n",
    "y = min_init['dbi']\n",
    "\n",
    "plt.plot(dfDBI['grid_size'], dfDBI['dbi'], linewidth=2.0)\n",
    "plt.plot(x, y, marker='o', markersize=3, color=\"blue\")\n",
    "plt.axvline(x=99, color='black')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig('./dbi-std-score-over-N.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# plot DB index over ratio of p and q \n",
    "\n",
    "dfDBI['ratio'] = dfDBI.apply(lambda row: np.float(row['p']) / np.float(row['q']), axis=1)\n",
    "\n",
    "min_score = dfDBI.dbi.min()\n",
    "min_init = dfDBI.loc[dfDBI['dbi'] == min_score]\n",
    "x = min_init['ratio']\n",
    "y = min_init['dbi']\n",
    "\n",
    "dfDBI = dfDBI.sort_values('ratio')\n",
    "\n",
    "plt.plot(dfDBI['ratio'], dfDBI['dbi'], linewidth=2.0)\n",
    "plt.plot(x, y, marker='o', markersize=3, color=\"blue\")\n",
    "plt.axvline(x=ratio, color='black')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig('./dbi-std-over-pq-ratio.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
