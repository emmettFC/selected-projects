{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Data Driven Yale: Analytical regionalization & Social media data\n",
    "        I: Aggregate weibo data for polygons \n",
    "        II: Do aggregations by an hourly and 20 minute basis\n",
    "'''\n",
    "\n",
    "# -- \n",
    "# dependancies \n",
    "\n",
    "from datetime import timedelta\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from dateutil import parser\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# io\n",
    "\n",
    "weibo_frame = pd.read_csv('../data/vtess-finite-npdefaults-9-11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# user defined functions\n",
    "\n",
    "def getCompId(row, bucket_minutes): \n",
    "    dif = (row['start_time'] - row['day_start']).total_seconds() / bucket_minutes\n",
    "    return int(dif / bucket_minutes) \n",
    "\n",
    "def getBucket(row): \n",
    "    r = row['time']\n",
    "    b = buckets.loc[(buckets['end_time'] >= r) & (buckets['start_time'] <= r)]\n",
    "    return int(b['bucket'])\n",
    "\n",
    "def buildXsn(row, n_day_buckets):\n",
    "    xsn = [0] * n_day_buckets \n",
    "    s = row['label']\n",
    "    n = row['day']\n",
    "    records = agg1.loc[(agg1['label'] == s) & (agg1['day'] == n)]\n",
    "    to_modify = xsn\n",
    "    indexes = list(records['comp_id'])\n",
    "    replacements = list(records['index'])\n",
    "    for i in range(len(indexes)):\n",
    "        # print(i)\n",
    "        to_modify[indexes[i]] = replacements[i]\n",
    "    return to_modify\n",
    "\n",
    "def isWeekday(row): \n",
    "    d = row['day_start']\n",
    "    if d.weekday() in [5,6]: \n",
    "        out = 'weekend'\n",
    "    else: \n",
    "        out = 'weekday'\n",
    "    return out\n",
    "\n",
    "def getAvgVec(row): \n",
    "    vecs = np.array(row['xsn'])\n",
    "    comb = np.mean(vecs, axis=0)\n",
    "    return comb\n",
    "\n",
    "def dayOfweek(row): \n",
    "    d = row['day_start']\n",
    "    return d.weekday()\n",
    "\n",
    "def getConcatVec(row): \n",
    "    items = row['avg_vec']\n",
    "    l = list(items[0])\n",
    "    for i in range(len(items)): \n",
    "        if i > 0: \n",
    "            l.extend(list(items[i]))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# convert time feild and reduce columns\n",
    "\n",
    "weibo_frame['time'] = weibo_frame.apply(lambda row: parser.parse(row['timestamp'].replace('\"','')), axis =1)\n",
    "wbf = weibo_frame[[u'unID', u'lon', u'lat', u'timestamp', u'labelInPoly',\n",
    "       u'label', u'center', u'polygon', u'time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# build and export vectors for n_polygon by 48 hour aggregation\n",
    "\n",
    "wbf.reset_index(inplace=True)\n",
    "tvect = wbf[['time', 'index']]\n",
    "buckets = tvect.set_index('time')\n",
    "buckets = buckets.resample('60T', how='count')\n",
    "buckets.reset_index(inplace=True)\n",
    "buckets['end_time'] = buckets.apply(lambda row: row['time'] + timedelta(minutes=59, seconds=59), axis=1)\n",
    "buckets.reset_index(inplace=True)\n",
    "buckets.columns = ['bucket', 'start_time', 'count', 'end_time']\n",
    "\n",
    "buckets['day'] = buckets.apply(lambda row: row['bucket'] / 24, axis=1)\n",
    "buckets['day_start'] = buckets.apply(lambda row: row['start_time'].replace(hour=0, minute=0, second=0), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# apply functions to bucket frame to join to weibo data\n",
    "\n",
    "buckets['comp_id'] = buckets.apply(lambda row: getCompId(row, 60), axis=1)\n",
    "wbf['bucket'] = wbf.apply(lambda row: getBucket(row), axis=1)\n",
    "\n",
    "weibo = pd.merge(wbf, buckets, on='bucket', how='left')\n",
    "weibo_60_minutes = weibo[[u'bucket', u'index', u'lon', u'lat', u'timestamp',\n",
    "       u'label', u'time', u'start_time', u'count',\n",
    "       u'day', u'comp_id', u'polygon',u'center']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# have to build frame of polygon-day given limit of data coverage\n",
    "\n",
    "days = list(buckets['day']) \n",
    "days = list(set(days))\n",
    "polygons = list(weibo['label'])\n",
    "polygons = list(set(polygons))\n",
    "\n",
    "frame = []\n",
    "for p in polygons: \n",
    "    for d in days: \n",
    "        out = {\n",
    "            'label' : p, \n",
    "            'day' : d\n",
    "        }\n",
    "        frame.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# aggregate data by polygon-day-hours \n",
    "\n",
    "agg1 = weibo.groupby(['label', 'day', 'comp_id']).agg({'index' : 'count'})\n",
    "agg1.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# create vectors with function \n",
    "\n",
    "Xsn = pd.DataFrame(frame)\n",
    "Xsn['xsn'] = Xsn.apply(lambda row: buildXsn(row, 24), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# join to dates and label by weekennd and weekday\n",
    "\n",
    "days = buckets[['day', 'day_start']].drop_duplicates() \n",
    "xsn = pd.merge(Xsn, days, on='day', how='left')\n",
    "xsn['weekday'] = xsn.apply(lambda row: isWeekday(row), axis=1)\n",
    "Xswkn = xsn.loc[xsn['weekday'] == 'weekend']\n",
    "Xswkd = xsn.loc[xsn['weekday'] == 'weekday']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# aggregate by weekend weekday\n",
    "\n",
    "Xswkn = Xswkn.groupby('label').agg({'xsn':(lambda x: list(x))})\n",
    "Xswkd = Xswkd.groupby('label').agg({'xsn':(lambda x: list(x))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# build average activity vectors for wknd/wkdy\n",
    "\n",
    "Xswkn['avg_vec'] = Xswkn.apply(lambda row: getAvgVec(row), axis=1)\n",
    "Xswkd['avg_vec'] = Xswkd.apply(lambda row: getAvgVec(row), axis=1)\n",
    "\n",
    "Xswkd.reset_index(inplace=True)\n",
    "Xswkn.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# join average vectors for each polygon to build one 48 hour dim vector of average activity\n",
    "\n",
    "Xswkn.columns = ['label', 'xsnWknd', 'avg_vec_wkn']\n",
    "df_vector = pd.merge(Xswkd, Xswkn, on='label', how='left')\n",
    "df_vector['combined'] = df_vector.apply(lambda row: list(row['avg_vec']) + list(row['avg_vec_wkn']), axis=1)\n",
    "df_spect = df_vector[['label', 'combined']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# Join to polygons and activity vectors and write to csv\n",
    "\n",
    "polys = weibo_frame[['label', 'center', 'polygon']].drop_duplicates()\n",
    "week_hour_vectors = pd.merge(df_spect, polys, on='label', how='left')\n",
    "'''can be tabbed back in for local copy; already run in this repo'''\n",
    "# week_hour_vectors.to_csv('../data/weekday-weekend-hour-vectors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# repeat process for day-hour level (0-6) activity vectors \n",
    "\n",
    "xsn['day_lab'] = xsn.apply(lambda row: dayOfweek(row), axis=1)\n",
    "Xsall = xsn.groupby(['label', 'day_lab']).agg({'xsn':(lambda x: list(x))})\n",
    "Xsall['avg_vec'] = Xsall.apply(lambda row: getAvgVec(row), axis=1)\n",
    "Xsall = Xsall.groupby('label').agg({'avg_vec':(lambda x: list(x))})\n",
    "Xsall['combined'] = Xsall.apply(lambda row: getConcatVec(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- \n",
    "# join to polygons and write to csv\n",
    "\n",
    "day_hour_vectors = pd.merge(Xsall, polys, on='label', how='left')\n",
    "day_hour_vectors = day_hour_vectors[['label', 'combined', 'center', 'polygon']]\n",
    "'''can be tabbed back in for local copy; already run for this repo'''\n",
    "# day_hour_vectors.to_csv('../data/day-hour-vectors.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
